---
title: 'Unsupervised Learning'
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Using unsupervised algorithms to segregate information from Getting information from  [2016 Global Health Observatory data](http://www.who.int/gho/publications/world_health_statistics/2016/en/).  

```{r WHS}
whsAnnBdatNum <- read.table("data\\whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whsAnnBdatNum[,c(1,4,7,10,17)])
pairs(whsAnnBdatNum[,c(1,4,7,10,17)])

# Transforming long names in smaller ones.
whs.rownames <- row.names(whsAnnBdatNum)
whs.rownames <- gsub("CentralAfricanRepublic", "AfricanRepublic", whs.rownames)
whs.rownames <- gsub("DemocraticPeople'sRepublicofKorea", "NorthKorea", whs.rownames)
whs.rownames <- gsub("DemocraticRepublicoftheCongo", "RepCongo", whs.rownames)
whs.rownames <- gsub("RepublicofKorea", "SouthKorea", whs.rownames)
whs.rownames <- gsub("TheformerYugoslavRepublicofMacedonia", "Macedonia", whs.rownames)
whs.rownames <- gsub("UnitedArabEmirates", "ArabEmirates", whs.rownames)
whs.rownames <- gsub("UnitedKingdom","UK", whs.rownames)
whs.rownames <- gsub("UnitedRepublicofTanzania","Tanzania", whs.rownames)
whs.rownames <- gsub("UnitedStatesofAmerica","USA", whs.rownames)

# Getting only the first 10 characters of country names
whs.rownames <- substr(whs.rownames, 1, 10)

rownames(whsAnnBdatNum) <- whs.rownames

```


```{r whsmeanvar}
# Create vectors with mean and variance.
whs.mean <- apply(whsAnnBdatNum, 2, mean)
whs.var <- apply(whsAnnBdatNum, 2, var)

# Plot the mean vs variance
plot(whs.mean, whs.var, log = "xy")
text(whs.mean, whs.var, names(whs.mean),  cex=0.5, pos=1, col="red")
```

**It is very hard to see all attributes on the plot, because most attribute names are overlapped. There is a linear dependence between the mean and the variance, this is verified in the plot. TOTPOP and INTINTDS are the top two attributes with the highest mean or variance.**

```{r whsrange}
# Disable scientific notation
options(scipen=999)

# Print mean range
print(paste("min:",range(whs.mean)[1],"  max:", range(whs.mean)[2]))  

# Print variance range
print(paste("min:",range(whs.var)[1],"  max:", range(whs.var)[2]))

options(scipen=0)
```

**As we can see above the range of means and variance is extremely wide. If we apply the PCA in the untransformed data we will have a distortion, since the PCA components follow the direction of the greatest variance.**

**Untransformed Data**
```{r whspcauntransformeddata, fig.width=9, fig.height=9}
# Disable warnings
options(warn=-1)

# 1 Untransformed
pcawhs.un <- prcomp(whsAnnBdatNum)
# 2
plot(pcawhs.un)
# 3
biplot(pcawhs.un, cex = 0.5)
# 4
plot(abs(pcawhs.un$rotation[,1:2]), log="xy")
text(abs(pcawhs.un$rotation[,1:2]), row.names(pcawhs.un$rotation), pos = 1, cex = 0.5, col="red" )
# 5
pcawhs.un.var <- pcawhs.un$sdev^2
pvewhs.un     <- pcawhs.un.var/sum(pcawhs.un.var)

# Enable Warning
options(warn=0)
```
**TOTPOP and INTINTDS have the largest loading for the first and second principal component. It is possible to observe that are the same variables that we had observed as the highest means and variances **


**Scaled data**
```{r whspcascaleddata, fig.width=9, fig.height=9}
# Working with Scaled data
pcawhs.sc <- prcomp(whsAnnBdatNum, scale = T)

plot(pcawhs.sc)
biplot(pcawhs.sc, cex=0.5)

plot(abs(pcawhs.sc$rotation[,1:2]), log="xy")
text(abs(pcawhs.sc$rotation[,1:2]), row.names(pcawhs.sc$rotation), pos = 1, cex = 0.5, col="red" )
pcawhs.sc.var <- pcawhs.sc$sdev^2
pvewhs.sc     <- pcawhs.sc.var/sum(pcawhs.sc.var)
```



**When the data is scaled you get more standardized information because the data is all transformed into a Z scale. This avoids the problem of data with heterogeneous metrics, for example, total population and the birth rate per 1000 inhabitants. As previously stated, TOTPOP and INTINTDS were the attributes that most influenced untransformed data, they have the highest values and variance. In the case of the scaled data are several, for example for PC1 we can mention NEONMORT, MORLT5YO and for PC2 would be HOMICIDE, MORTRAN. We can relate that high values of PC1 are more related to mortality rates and PC2 related to homicidies. We also see a relationship between life expectancy and the proportion of the population with access to clean fuels, improved sanitation and potable water.**

```{r whsanalysisbycountry, fig.width=8, fig.height=8}
# Plot using the first two columns of the 'x'
plot(pcawhs.sc$x[,1:2])
text(pcawhs.sc$x[,1:2], row.names(pcawhs.sc$x[,1:2]), pos = 1, cex = 0.5, col = "red")

# Plot using only the countries that we want analyse
countries = c("USA", "UK", "China", "India", "Mexico", "Australia", "Israel", "Italy", "Ireland", "Sweden")
plot(pcawhs.sc$x[countries,1:2])
text(pcawhs.sc$x[countries,1:2], row.names(pcawhs.sc$x[countries,1:2]), pos = 1, cex = 0.5, col = "red")
```

**Analyzing the countries in question we see that more developed countries like Australia, USA, Italy, UK, Sweden, Ireland are closer to a high life expectancy. India has high mortality rates, while Mexico is closer to homicide indicators.**

# K-means clustering -> Testing different clusters size

```{r whskmeanscluster, fig.width=8, fig.height=8}
# 2 clusters
km.out <- kmeans(scale(whsAnnBdatNum),2)
summary(km.out)
plot(pcawhs.sc$x[,1:2],col=km.out$cluster)
text(pcawhs.sc$x[,1:2],row.names(pcawhs.sc$x[,1:2]), pos = 1, cex = 0.5)


# 3 clusters
km.out <- kmeans(scale(whsAnnBdatNum),3)
summary(km.out)
plot(pcawhs.sc$x[,1:2],col=km.out$cluster)
text(pcawhs.sc$x[,1:2],row.names(pcawhs.sc$x[,1:2]), pos = 1, cex = 0.5, col = "pink4")


# 4 clusters
km.out <- kmeans(scale(whsAnnBdatNum),4)
summary(km.out)
plot(pcawhs.sc$x[,1:2],col=km.out$cluster)
text(pcawhs.sc$x[,1:2],row.names(pcawhs.sc$x[,1:2]), pos = 1, cex = 0.5, col = "seagreen4")
```

**Using only two clusters a very superficial grouping was made, in which several countries with different situations were placed in the same group, for example Australia and Honduras. Using 3 clusters it was possible to separate the groups on the left (life expectancy), the center (medium indicators) and the right (mortality), however countries with high and low homicide indicators were placed in the same group (Hungary and Venezuela). With 4 clusters few changes were identified.**

# K-means with different clusters size

``` {r whskmeansnstart, fig.width=10, fig.height=7}
# Set the quantity of clustes to 4
iCluster <- 4

# Loop to run 1 and 100 nstart
for (n.start in c(1,100)) {
  # Loop to run 1, 2 and 3 seeds
  for (n.seed in c(1, 2, 3)) {
    set.seed(n.seed)
    km.out <- kmeans(scale(whsAnnBdatNum), center = iCluster, nstart = n.start)
    plot(pcawhs.sc$x[,1:2],col=km.out$cluster, main = paste("NSTART =", n.start, "SEED = ", n.seed))
    text(pcawhs.sc$x[,1:2],row.names(pcawhs.sc$x[,1:2]), pos = 1, cex = 0.5, col = "darkslategray")
    
    # Print the tot.tot.withinss and betweenss
    print("tot.withinss")
    print(km.out$tot.withinss)
    print("betweenss")
    print(km.out$betweenss)
  }
}

```

**Using NSTART = 1, it was possible to detect a different cluster at each RNG reset, because the centers were randomly selected only once. The values of tot.withinss and betweenss were different for each execution.**

**Using NSTART = 100, the results were the same since the kmeans function had 100 chances to find the optimal center. The values of tot.withinss and betweenss were exactly the same.**


# Testing Hierarchical clustering - hierachical clustering by different linkages (10 points)

``` {r whshclust, fig.width = 14, fig.height = 7}
x <- scale(whsAnnBdatNum)
hc.complete     <- hclust (dist(x), method ="complete")
hc.average      <- hclust (dist(x), method ="average")
hc.single       <- hclust (dist(x), method ="single")
hc.complete.un  <- hclust (dist(whsAnnBdatNum), method ="complete")

old.par <- par(cex = 0.5)
plot(hc.complete    , main =" Complete Linkage ")
rect.hclust(hc.complete, 4)
plot(hc.average     , main =" Average Linkage " )
rect.hclust(hc.average, 4)
plot(hc.single      , main =" Single Linkage "  )
rect.hclust(hc.single, 4)
plot(hc.complete.un , main =" Complete Linkage Untransformed "  )
rect.hclust(hc.complete.un, 4)


par(old.par)

```

**Clustering by hierarchy is used when the number of clusters is not well known, in which case it has a behavior different from Kmeans. I believe that the complete linkage had a better result than the other two (single and average). Average also grouped interestingly some countries, however the linkage single merged many countries into one group only, making the analysis very complex.**
**Clustering of unprocessed data resulted in incoherent clusters, also several countries were in the same group.**

## Comparing k-means and hierarchical clustering

**Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and "complete" linkage.**

```{r whscutree}
hc.cluster <- cutree(hc.complete, 4)
hc.cluster
table(hc.cluster, km.out$cluster  )
```

**According to the table above, we can see that due to its structure the clustering by hierarchy could not group very well using 4 clusters. Clusters 3 and 4 remained with only 1 country. In contrast, the kmeans algorithm was able to separate the countries in a more equivalent way into 4 clusters.**


